{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e286120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd380b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# System path for imports\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "PROJECT_ROOT='./'\n",
    "import sys\n",
    "sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f18e8e",
   "metadata": {},
   "source": [
    "### Branch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb3c1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2667, 0.2856, 0.2326,  ..., 0.0130, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 401408])\n",
      "torch.Size([512, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Driver code\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from architecture.cnn_architecture import CNN_ARCHITECTURE\n",
    "from configs.cnn_branch_config import cnn_experiment_1\n",
    "\n",
    "model = CNN_ARCHITECTURE(cnn_experiment_1['model_args']['input_size'], cnn_experiment_1['model_args']['hidden_layers'], cnn_experiment_1['model_args']['activation'], cnn_experiment_1['model_args']['norm_layer'], cnn_experiment_1['model_args']['drop_prob'])\n",
    "model.eval()\n",
    "\n",
    "input = torch.randn(1,3,224,224)\n",
    "\n",
    "final_features = model(input)\n",
    "\n",
    "print(final_features)\n",
    "print(final_features.shape)\n",
    "\n",
    "features_in_shape = final_features.view(512, 28, 28)\n",
    "print(features_in_shape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650c44d",
   "metadata": {},
   "source": [
    "### Branch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feea971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/HLCV/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS token shape: torch.Size([1, 768])\n",
      "All token embeddings shape: torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "from architecture.vit_architectire import VIT_ARCHITECTURE\n",
    "from configs.cnn_branch_config import cnn_experiment_1\n",
    "\n",
    "vit_model = VIT_ARCHITECTURE(cnn_experiment_1['model_args']['model_name'])\n",
    "\n",
    "\n",
    "vit_features = vit_model.extract_features(\"./Blur.png\")\n",
    "print(\"CLS token shape:\", vit_features['cls'].shape)\n",
    "print(\"All token embeddings shape:\", vit_features['all'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f9555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120216a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
