{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e286120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd380b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# System path for imports\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "PROJECT_ROOT='./'\n",
    "import sys\n",
    "sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ae4d6",
   "metadata": {},
   "source": [
    "## Experiment 1: CNN + Transformer Hybrid Architecture\n",
    "\n",
    "Contains the hybrid architecture with element wise product of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37e98c",
   "metadata": {},
   "source": [
    "### CNN-ViT Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c57e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Training Loop\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def train(model, dataloader, optimizer, epochs=5, log_file='training_log.txt', best_model_dir='best_models/'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    os.makedirs(best_model_dir, exist_ok=True)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Open log file\n",
    "    with open(log_file, 'w') as log:\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for blurry, sharp in dataloader:\n",
    "                blurry = blurry.to(device)\n",
    "                sharp = sharp.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(blurry)\n",
    "                loss = criterion(output, sharp)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "            # Save best model\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(model.state_dict(), os.path.join(best_model_dir, 'best_model.pth'))\n",
    "\n",
    "            # Logging\n",
    "            log_line = f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\\n\"\n",
    "            print(log_line.strip())\n",
    "            log.write(log_line)\n",
    "            log.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1321ea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/HLCV/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Features: torch.Size([512, 784]) = 512 channels, 28.0 height, width\n",
      "Attention shape: torch.Size([1, 197, 197])\n",
      "Attention Map reshaped: torch.Size([1, 1, 197, 197])\n",
      "Attention Map after interpolation: torch.Size([1, 1, 512, 784])\n",
      "Attention Map resized: torch.Size([1, 512, 512, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAADyCAYAAAArx4ypAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAADbdJREFUeJzt3X1MlXUfx/HP8YmDhBCioBKIpM7HRO1hZBMyn0jdEhWnIjR1mltuOmpTKMA7c9WcfzRMlwnOMh+2nJla4QJrw8YYTqdRWXnUZoiSYybWZvvdfzjO7YmjXywS636/Nv4417nO9ftdl/PN9SDocc45AQBuqUN7TwAA7nWEEgAMhBIADIQSAAyEEgAMhBIADIQSAAyEEgAMhBIADISyHZSWlsrj8fi/vF6vYmNjlZaWprVr16q+vv4vb7u6utpcNycnR3379v3TY91tPp9PHo9HpaWlrVq/trZWOTk5io+PV5cuXRQdHa309HQdPHjwL81jw4YNrZ7DX9XU1KTCwkJVVFTclfEQHKFsRyUlJTpy5IjKyspUXFysESNG6LXXXtOgQYN06NCh9p7eP9oHH3yg5ORkVVVV6aWXXtKhQ4f01ltvSZLS09P14osv/ult3+1QFhUVEcp21qm9J/D/bOjQoRo9erT/dUZGhpYvX64xY8Zo+vTpOnXqlGJiYtpxhnemqalJXbt2DfretWvXFBoaelfm8f333ysrK0vDhg1TRUWFwsLC/O/NnDlTzz33nN544w2NHDlSs2fPvitzwj8bZ5T3mPj4eK1bt05XrlzRpk2bAt6rrq7WtGnTFBUVJa/Xq+TkZO3atSvodi5fvqxnn31WUVFRCgsL09SpU/XDDz/cduzbXdp6PB4VFhb6XxcWFsrj8aimpkYzZszQ/fffr6SkJElS3759NWXKFP9ZndfrVVFRkSSprq5OixcvVlxcnLp06aLExEQVFRXp+vXrAeOdP39es2bNUnh4uCIiIpSZmam6ujrr8EmS1q9fr6amJr355psBkWy2bt06RUZGas2aNS3254+ab2X4fD7/vp08eVKHDx/23zppvn1RUVEhj8ejd999VytWrFBsbKxCQ0M1duxYHT16NGC7qampSk1NbTHezbdDfD6fevToIUkqKiryj5eTk9Oq44C2wxnlPSg9PV0dO3bU559/7l9WXl6uSZMm6dFHH9XGjRsVERGhHTt2KDMzU01NTS3+8ixYsEDjx4/X9u3bde7cOeXn5ys1NVXHjx9XZGRkm811+vTpmj17tpYsWaKrV6/6l9fU1Ki2tlb5+flKTExUWFiY6urq9Mgjj6hDhw56+eWXlZSUpCNHjuiVV16Rz+dTSUmJpBtnn0899ZTOnz+vtWvXasCAAdq/f78yMzNbNaeysjLFxMToscceC/p+165dNWHCBO3atUt1dXWKjY1t9f7u2bNHM2bMUEREhDZs2CBJCgkJCVhn1apVGjlypDZv3qzGxkYVFhYqNTVVR48eVb9+/Vo9Vq9evfTxxx9r0qRJWrBggRYuXChJ/nji7iGU96CwsDBFR0fr/Pnz/mVLly7VkCFD9Nlnn6lTpxt/bBMnTtSlS5e0atUqzZ8/Xx06/O8CYfTo0XrnnXf8r4cMGaLHH39cxcXFysvLa7O5Zmdn+88Wb1ZfX6+vvvpKAwYM8C9bsmSJLl++rJMnTyo+Pl6SNG7cOIWGhio3N1cvvPCCBg8erK1bt6q2tlZ79+7VtGnTJEkTJkzQtWvX9Pbbb5tzOnv2rEaMGHHbdRITE/3r3kkok5OTFRoaqm7dut0yxD169NCePXv8Z6hjxoxR//79tXbt2lbNv1lISIhGjRolSYqLi7vlePj7cel9j7r514R+9913+vrrrzV37lxJ0vXr1/1f6enp+umnn/TNN98EfL553WYpKSlKSEhQeXl5m84zIyMj6PLhw4cHRFKSPvroI6Wlpal3794B+zB58mRJ0uHDhyXdOHsODw/3R7LZnDlz2mzezcc32OX2XzVnzpyA7SYkJCglJaXNjz3uHs4o70FXr15VQ0ODhg0bJkm6cOGCJCk3N1e5ublBP3Pp0qWA18HOkmJjY9XQ0NCmc+3Vq1erl1+4cEH79u1T586dg36meR8aGhqCPsRq7ZlffHy8Tp8+fdt1mu85PvDAA63a5p241bE/duxYm4+Fu4NQ3oP279+v33//3X+zPzo6WpK0cuVKTZ8+PehnBg4cGPA62IOPuro6Pfjgg7cc1+v1SpJ+++23gOW3i+utzsiCLY+Ojtbw4cMDHqLcrHfv3pKk7t27q6qqqsX7rX2YM378eBUXF+vLL78Merna1NSksrIyDR061B+1m/f95nuOf/wG1Bq3Ovbdu3f3v/Z6vWpsbGyx3p8ZD38/Lr3vMWfPnlVubq4iIiK0ePFiSTci2L9/fx07dkyjR48O+hUeHh6wnffeey/gdWVlpc6cORP0SWuzmJgYeb1eHT9+PGD53r1722TfpkyZohMnTigpKSnoPjSHMi0tTVeuXNGHH34Y8Pnt27e3apzly5crNDRUzz//fMADpma5ubm6fPmy8vPz/cuanzT/cd/37dvX4vMhISG6du3aLcd///33A26dnDlzRpWVlQHHvm/fvvr2228Dvik1NDSosrKyxViSbjse/n6cUbajEydO+O/T1dfX64svvlBJSYk6duyoPXv2BDzd3LRpkyZPnqyJEycqJydHffr00c8//6za2lrV1NRo9+7dAduurq7WwoULNXPmTJ07d055eXnq06ePli5desv5eDwezZs3T1u2bFFSUpIeeughVVVVtTpQltWrV6usrEwpKSlatmyZBg4cqF9//VU+n08HDhzQxo0bFRcXp/nz52v9+vWaP3++1qxZo/79++vAgQP65JNPWjVOUlKStm3bprlz5+rhhx/WihUrNHDgQF24cEFbtmzRwYMHlZubG/AUPT09XVFRUVqwYIFWr16tTp06qbS0VOfOnWux/WHDhmnHjh3auXOn+vXrJ6/X679NIt14kPXMM89o0aJFamxsVEFBgbxer1auXOlfJysrS5s2bdK8efO0aNEiNTQ06PXXX1e3bt0CxgoPD1dCQoL27t2rcePGKSoqStHR0f+on6j6V3C460pKSpwk/1eXLl1cz5493dixY92rr77q6uvrg37u2LFjbtasWa5nz56uc+fOLjY21j355JNu48aNLbb96aefuqysLBcZGelCQ0Ndenq6O3XqVMD2srOzXUJCQsCyxsZGt3DhQhcTE+PCwsLc1KlTnc/nc5JcQUGBf72CggInyV28eLHFPBMSEtzTTz8ddB8uXrzoli1b5hITE13nzp1dVFSUGzVqlMvLy3O//PKLf70ff/zRZWRkuPvuu8+Fh4e7jIwMV1lZ6SS5kpIS4wjfcPLkSZedne3i4uL8Y02aNMnt378/6PpVVVUuJSXFhYWFuT59+riCggK3efNmJ8mdPn3av57P53MTJkxw4eHhTpL/GJaXlztJbtu2bW7ZsmWuR48eLiQkxD3xxBOuurq6xXhbt251gwYNcl6v1w0ePNjt3Lkz6J/JoUOHXHJysgsJCXGSXHZ2dqv2H23H4xz/CyPQFioqKpSWlqbdu3drxowZ7T0dtCHuUQKAgVACgIFLbwAwcEYJAAZCCQAGQgkABkIJAIZW/2TOf/LzpLb/RSsA0D6c9NIrwX/vwB/d0Y8w0kkA/xZ38s99uPQGAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAA6EE8H/H3eH6nf7WrQPAv4DHOUf+AOA2uPQGAAOhBAADoQQAA6EEAAOhBAADoQQAA6EEAAOhBAADoQQAw38Bv91a2LVoksAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN_VIT_HYBRID_ARCHITECTURE()\n\u001b[0;32m---> 15\u001b[0m model(image)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/HLCV/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/HLCV/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MS/Saarland University/Semester 1/HLCV/Project/architecture/cnn_vit_hybrid_architecture.py:95\u001b[0m, in \u001b[0;36mCNN_VIT_HYBRID_ARCHITECTURE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m attention_map \u001b[38;5;241m=\u001b[39m vit_attention\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# -> [12, 197, 197]\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Extract attention from CLS token (index 0) to all others\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Shape: [12, 197]\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m cls_attn \u001b[38;5;241m=\u001b[39m attention_map[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# [num_heads, tokens]\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Drop the CLS token itself for visualization (optional)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m cls_attn \u001b[38;5;241m=\u001b[39m cls_attn[:, \u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# [12, 196]\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Driver Code\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "from architecture.cnn_vit_hybrid_architecture import CNN_VIT_HYBRID_ARCHITECTURE\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "raw_image = Image.open(\"./Blur.png\").convert('RGB')\n",
    "image = transform(raw_image)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# Run experiments\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "model = CNN_VIT_HYBRID_ARCHITECTURE()\n",
    "model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea57b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
